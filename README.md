# Slack with Foundation Models

[![Tests](https://github.com/honnuanand/Slack-with-Foundation-Models/actions/workflows/tests.yml/badge.svg)](https://github.com/honnuanand/Slack-with-Foundation-Models/actions/workflows/tests.yml)
[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![OpenAI Compatible](https://img.shields.io/badge/OpenAI-Compatible-green)](https://platform.openai.com/docs)
[![Databricks](https://img.shields.io/badge/Databricks-Ready-orange)](https://databricks.com)
[![Slack](https://img.shields.io/badge/Slack-Bot-4A154B?logo=slack)](https://api.slack.com)

Connect Slack to Databricks and other LLM providers that are compatible with the OpenAI API format for connectivity.

This project provides both a **Slack bot** and **CLI interface** to chat with Foundation Models from providers like Databricks, OpenAI, and any other service that supports the OpenAI-compatible API format.

## üéØ What This Does

- **Slack Integration**: Chat with Foundation Models directly in Slack using Bolt for Python
- **CLI Interface**: Terminal-based chat interface for quick testing and development
- **OpenAI-Compatible**: Uses the standard OpenAI API format, making it easy to switch between providers
- **Flexible Provider Support**: Works with Databricks Foundation Models, OpenAI, or any OpenAI-compatible service

## üöÄ Quick Start

Choose your path:

1. **CLI Only** ‚Üí See [QUICKSTART.md](QUICKSTART.md) or [CLI_README.md](CLI_README.md)
2. **Slack Bot with Databricks** ‚Üí Continue below
3. **Testing with OpenAI** ‚Üí See [TESTING_WITH_OPENAI.md](TESTING_WITH_OPENAI.md)
4. **Not sure which file to use?** ‚Üí See [WHICH_FILE_TO_USE.md](WHICH_FILE_TO_USE.md)

## ‚ú® Features

- ü§ñ **Multiple Providers**: Databricks, OpenAI, or any OpenAI-compatible LLM service
- üîÑ **Model Switching**: Switch between different models on the fly
- üí¨ **Conversation History**: Maintains context across multiple turns
- ‚ö° **Real-time Responses**: Fast, streaming responses
- üé® **Color-Coded Output**: Enhanced CLI experience with color formatting
- üì± **Slack Integration**: Full Slack bot with mentions, DMs, and slash commands
- üîå **Standard API**: Uses OpenAI's Chat Completions API format

## üõ†Ô∏è Available Implementations

| File | Interface | Providers | Best For |
|------|-----------|-----------|----------|
| `chat_cli.py` | Terminal | Databricks only | Quick local testing with Databricks |
| `chat_cli_openai.py` | Terminal | Databricks OR OpenAI | Flexible development/testing |
| `app.py` | Slack | Databricks only | Production Slack bot with Databricks |
| `app_openai.py` | Slack | Databricks OR OpenAI | Testing Slack bot without Databricks |

## üìã Setup - Slack Bot with Databricks

### 1. Install Dependencies

```bash
# Create virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate

# Install packages
pip install -r requirements.txt
```

### 2. Create a Slack App

1. Go to [https://api.slack.com/apps](https://api.slack.com/apps)
2. Click "Create New App" ‚Üí "From scratch"
3. Name your app (e.g., "AI Assistant")
4. Select your workspace

### 3. Configure Slack App

#### Bot Token Scopes
Add these OAuth scopes under "OAuth & Permissions":
- `app_mentions:read`
- `chat:write`
- `im:history`
- `im:read`
- `im:write`
- `channels:history`
- `groups:history`
- `mpim:history`

#### Event Subscriptions
Enable events and subscribe to:
- `app_mention`
- `message.im`

#### Socket Mode
1. Enable Socket Mode under "Socket Mode"
2. Generate an App-Level Token with `connections:write` scope

#### Install App
Install the app to your workspace under "Install App"

### 4. Configure Environment Variables

```bash
cp .env.example .env
```

Edit `.env` with your credentials:
```env
# For Databricks
MODE=databricks
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com
DATABRICKS_TOKEN=dapi...

# For Slack
SLACK_BOT_TOKEN=xoxb-your-bot-token
SLACK_APP_TOKEN=xapp-your-app-token
```

**Getting Databricks Credentials:**
1. **Host**: Your Databricks workspace URL
2. **Token**: Generate from User Settings ‚Üí Developer ‚Üí Access Tokens

### 5. Run the Slack Bot

```bash
source venv/bin/activate
python app.py
```

You should see:
```
‚ö°Ô∏è Bolt app is running with Databricks Foundation Models!
```

## üí° Usage

### In Slack Channels
Mention the bot with your question:
```
@AI Assistant What is machine learning?
```

### In Direct Messages
Just type your question:
```
Explain transformers in AI
```

### Commands
- **Get help**: Say "help" or "models"
- **Switch models**: Say "use llama-70b" or "use claude-sonnet"
- **Clear history**: Say "clear" to reset conversation context

## üîß Testing with OpenAI Instead

Want to test the Slack bot without Databricks access? You can use OpenAI instead!

1. Get an OpenAI API key from https://platform.openai.com/api-keys
2. Update your `.env`:
   ```env
   MODE=openai
   OPENAI_API_KEY=sk-your-key-here
   ```
3. Run: `python app_openai.py`

See [TESTING_WITH_OPENAI.md](TESTING_WITH_OPENAI.md) for complete instructions.

## üèóÔ∏è How It Works - The OpenAI API Format

All implementations use the **OpenAI Chat Completions API** format:

```python
from openai import OpenAI

# For Databricks
client = OpenAI(
    api_key=DATABRICKS_TOKEN,
    base_url=f"{DATABRICKS_HOST}/serving-endpoints"
)

# For OpenAI
client = OpenAI(
    api_key=OPENAI_API_KEY
)

# Same API call for both!
response = client.chat.completions.create(
    model="model-id",
    messages=[{"role": "user", "content": "Hello"}],
    max_tokens=1000,
    temperature=0.7
)
```

**Same syntax, different endpoint** - that's the power of the OpenAI-compatible API format!

## üìö Available Models

### Databricks Foundation Models
- Llama 4 Maverick
- Llama 3.3 70B Instruct
- Llama 3.1 405B Instruct
- Claude Sonnet 4.5
- Claude Opus 4.1
- GPT OSS 120B
- And more...

### OpenAI Models
- GPT-4o
- GPT-4o Mini
- GPT-4 Turbo
- GPT-3.5 Turbo

*The actual available models depend on your workspace/account configuration*

## üìÇ Project Structure

```
Slack-with-Foundation-Models/
‚îú‚îÄ‚îÄ app.py                    # Slack bot (Databricks only)
‚îú‚îÄ‚îÄ app_openai.py             # Slack bot (flexible provider)
‚îú‚îÄ‚îÄ chat_cli.py               # CLI (Databricks only)
‚îú‚îÄ‚îÄ chat_cli_openai.py        # CLI (flexible provider)
‚îú‚îÄ‚îÄ example_usage.py          # Simple API example
‚îú‚îÄ‚îÄ requirements.txt          # All dependencies
‚îú‚îÄ‚îÄ requirements-cli.txt      # CLI-only dependencies
‚îú‚îÄ‚îÄ .env.example              # Environment template
‚îú‚îÄ‚îÄ README.md                 # This file
‚îú‚îÄ‚îÄ CLI_README.md             # CLI-specific documentation
‚îú‚îÄ‚îÄ QUICKSTART.md             # Quick start guide
‚îú‚îÄ‚îÄ TESTING_WITH_OPENAI.md    # OpenAI testing guide
‚îî‚îÄ‚îÄ WHICH_FILE_TO_USE.md      # Decision guide
```

## üîç Troubleshooting

### Slack Bot Issues
- **Bot doesn't respond**: Check Socket Mode is enabled and bot is invited to channel
- **Permission errors**: Review bot token scopes and reinstall app
- **Authentication errors**: Verify SLACK_BOT_TOKEN and SLACK_APP_TOKEN

### Databricks Issues
- **Invalid credentials**: Check DATABRICKS_HOST and DATABRICKS_TOKEN
- **Model not found**: Verify the model endpoint exists in your workspace
- **Access denied**: Ensure workspace has Foundation Models enabled

### OpenAI Issues
- **Invalid API key**: Verify key at https://platform.openai.com/api-keys
- **Rate limits**: Check your usage limits and billing

## üéì Learn More

- [Databricks Foundation Models Documentation](https://docs.databricks.com/en/machine-learning/foundation-models/index.html)
- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)
- [Slack Bolt for Python](https://slack.dev/bolt-python/)

## üìù License

MIT

## ü§ù Contributing

Contributions welcome! This project demonstrates how to:
- Build Slack bots with Foundation Models
- Use OpenAI-compatible APIs
- Switch between different LLM providers
- Maintain conversation context
- Handle streaming responses

Feel free to extend it with additional providers or features!
